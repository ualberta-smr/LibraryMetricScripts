'''
This script is no longer used due to the limits of the API. Instead, we are going to depend on the github dependency graph
This script searches the 1500 repositories in Top_Repo.txt for import statements of all the library packages in library.txt
Requires:   A configuration file called GitHubSearch.ini
            Ensure a GitHub token generated by your account is in the configuration account so that the script may connect to github 
            
Output:     A text file called popularity_results.txt which has each library along with a number representing how many distinct repositories it was in 
'''

import random
from github import Github, GithubException
import json  
from scripts.CommonUtilities import Common_Utilities
from scripts.SharedFiles.utility_tool import read_json_file


def sleep(github):
  github_limits = github.get_rate_limit()
  if github_limits.core.remaining == 1:
    Common_Utilities.go_to_sleep("API hour limit exceeded,Go to sleep for ", 3600)

  if github_limits.search.remaining == 0:
    Common_Utilities.go_to_sleep("API minute limit exceeded,Go to sleep for ", 61)

#This is where the search happens, an api query is used to collect results. 
#The query looks like this: "import LIBRARY-NAME" language:java repo:REPO-NAME
#For each Query, if we get ANY results then that means the library was used in that repo
#If we get ZERO results, it means it was not used in that repo
def search_code_in_repo(query, github, quick_sleep, error_sleep, max_size, Repo_Array):
   
  roll_back = True
  while roll_back:
    roll_back = False
    frequency = 0

    try:
    
      #check if we need to sleep because we exceeded rate limits
      sleep(github)

      index = 0
      tracking_counter = 0
      arraysize = len(Repo_Array)     
      while index < arraysize:      
        try:
          tracking_counter += 1                
          query_final = query + " repo:"+Repo_Array[index] 
          index += 1
          msg = str(index) + " out of " + str (arraysize) + " Query : " + query_final
          print (msg)
          result = None
          result = github.search_code(query_final)        
          num_found = result.totalCount      
          if  num_found > 0:
            frequency += 1      
          if tracking_counter % 15 == 0:
            Common_Utilities.go_to_sleep("Force to sleep after every 15 requests, Go to sleep for ", quick_sleep)          
        except GithubException:             
          index -= 1
          Common_Utilities.go_to_sleep("Error: Internal abuse detection mechanism detected,Go to sleep for ", error_sleep)
      
    except GithubException:
      Common_Utilities.go_to_sleep("Error: abuse detection mechanism detected,Go to sleep for ", error_sleep)
      roll_back = True # -1 means a problem detected and we need to re-read the same pages again after sleep. no change of date
   
  return frequency 
   
def read_libraries(file_path):
  libdict = {}
  f = read_json_file(file_path)
  for line in f:
    libdict[line['Package']]=line['FullRepoName']          
            
  return libdict
  
def send_totals_to_file(output_file, keyword, num_found):
  output_file = open(output_file, "a")  
  output_file.write(keyword + ":" + str(num_found) + "\n")
  output_file.close()  

def read_repos():
    repo_array = []
    with open("scripts/Popularity/Top_Repo.txt", "r") as f:
        for line in f:          
            repo_array.append(line.rstrip())
    return repo_array
  
def search_top_repos():

    print("Searching for imports in top repos...")
    
    config_dict = Common_Utilities.read_config_file() # read all config data
    repo_array = read_repos()    
    
    quick_sleep = int (config_dict["QUICK_SLEEP"]) # regular sleep after each iteration
    error_sleep = int (config_dict["ERROR_SLEEP"]) # Sleep after a serious issue is detected from gitHib, should be around 10min, ie 600 sec
    max_size = int (config_dict["MAXSIZE"]) # max number of results returned per gitHub call, for now it is 1500, but could be changed in the future
    
    github = None
    github = Github(config_dict["TOKEN"])   # pass the connection token 
    
    library_dict = read_libraries(config_dict["LIBRARY_LIST"]) # read all libraries to search against

    output_file_name = config_dict["POPULARITY_OUTPUT_FILE"] # this is the output file that we are going to send libraries with their total counts to
    
    output_file = open(output_file_name, "w")  
    output_file.close()  

    lib_counter = 1;    
    for keyword,repo in library_dict.items():  
      
      if lib_counter % 10 == 0:
         Common_Utilities.go_to_sleep("Sleeping after 10 libraries ", 4000)
      query = "\"import " + keyword + "\" "  + config_dict["IMPORT_SEARCH_QUERY"]  
      frequency = search_code_in_repo(query,github, quick_sleep, error_sleep, max_size, repo_array) 
      send_totals_to_file(output_file_name, repo, frequency )
      lib_counter += 1
       
    print ("\n Finally ..... Execution is over \n")    
    
if __name__ == "__main__":
    search_top_repos()
