'''
This script searches the 1500 repositories in Top_Repo.txt for import statements of all the library packages in library.txt
Requires:   A configuration file called Config.json
            Ensure a GitHub token generated by your account is in the configuration account so that the script may connect to github 
            
Output:     A text file called popularity_results.txt which has each library along with a number representing how many distinct repositories it was in. The name
          of the file can be changed in the configuration file as well
'''

import random
from github import Github, GithubException
import json  
from CommonUtilities import Common_Utilities

QUERY_SIZE = 20

#This makes the utility_tool visible from this file
import sys
sys.path.append('../')
from SharedFiles.utility_tool import read_json_file

def get_repo_name(code_file):
  '''
  Returns the repo name of the given code file
  '''
  return code_file.repository.full_name

class CodeSearch:

  def __init__(self):
    self.config_dict = Common_Utilities.read_config_file() # read all config data 
    self.github = Github(self.config_dict["TOKEN"])
    self.base_query = self.config_dict["IMPORTSEARCH"]
    self.repositories = []
    self.libdict = {}
    self.writer = None
    self.output_file = self.config_dict["POPULARITY_OUTPUT_FILE"]
    self.quick_sleep = int (self.config_dict["QUICK_SLEEP"]) # regular sleep after per minute API limit timeout
    self.error_sleep = int (self.config_dict["ERROR_SLEEP"]) # Sleep after a serious issue is detected from gitHib, should be around 10min, ie 600 sec 

  def read_libraries(self):
    '''
    Read list of repositories whose code will be queried
    '''
    libdict = {}

    lib_entries = read_json_file(self.config_dict["LIBRARY_LIST"])
    for entry in lib_entries:
      libdict[entry['Package']]=entry['FullRepoName']          

    self.libdict = libdict 

  def read_repositories(self):
    '''
    Read list of repositories from phase 1 whose code will be queried
    '''
    repositories = set()
    with open("Popularity/Top_Repo.txt", "r") as input_file:
      for repo_name in input_file:          
          repositories.add(repo_name.rstrip('\n'))

      input_file.close()

    self.repositories = list(repositories)

  def send_totals_to_file(self,lib_keyword, num_repos_found):
    '''
    Given the results of the code search, write the total number of repos
    for a given library
    '''
    self.writer.write(lib_keyword + ":" + str(num_repos_found) + "\n")

  def close_output_file(self):
    self.writer.close()

  def search_code_in_repos(self, lib_query, repos):
    '''
    Appends a list of given repos to the base_query
    and uses the Github search API to search those repos 
    '''
    query = lib_query

    for repo in repos:
      query += " repo:" + repo

    return self.github.search_code(query)

  def _get_search_rate(self):
    rate_limit = self.github.get_rate_limit()
    return rate_limit.search

  def _get_unique_repo_cnt(self, code_results, num_repos_queried):


    cnt_processed = 0
    unique_repos = set()
    repos_each_time = 50

    try:
      results_size = code_results.totalCount
    except GithubException:
          Common_Utilities.go_to_sleep("Abuse detected, Going to sleep for ", self.error_sleep)

    start_index = 0
    end_index = repos_each_time
          
    while start_index < results_size:
      try:
        if end_index > results_size:
          end_index = results_size - 1

        print(len(unique_repos), "Repos found so far, now results checking from ", start_index, " to ", end_index)
        print(f'You have {self._get_search_rate().remaining}/{self._get_search_rate().limit} API calls remaining')

        sub_results = code_results[start_index:end_index]
        try: 
          unique_repos.update(set(map(get_repo_name, sub_results)))
          num_repos_found = len(unique_repos)
          print("got repo names")
          if(num_repos_found == num_repos_queried):
            print("Found max num of repos.. breaking!")
            break

          start_index += repos_each_time
          end_index = start_index + repos_each_time

          # if self._get_search_rate().remaining < 2:
          #   Common_Utilities.go_to_sleep("Processed 300 repos, but rate about to get exceeded, Go to sleep for ", self.quick_sleep)          
          # else:
          Common_Utilities.go_to_sleep("Processed 300 repos.. Going to sleep for ", self.quick_sleep)

        except GithubException:
            Common_Utilities.go_to_sleep("API limit exceeded, Going to sleep for ", self.quick_sleep)
            continue
      except GithubException:
        Common_Utilities.go_to_sleep("Abuse detected, Going to sleep for ", self.error_sleep)
        continue

    return len(unique_repos)

  def search_code(self, import_phrase):
    '''
    Creates a sublist of the available repositories given the
    start and end indices, issues the query, and then writes the
    results to the csv file
    '''

    print("===== starting queries for ", import_phrase)
    num_of_repos =  len(self.get_repositories())
    print ('total repos to query',num_of_repos)

    lib_query = import_phrase + self.base_query
    
    start_index = 0
    frequency = 0
    end_index = start_index + QUERY_SIZE - 1

    if end_index > num_of_repos:
      end_index = num_of_repos - 1
      
    while end_index < num_of_repos:
      try:
        print ('start', start_index)
        print ('end', end_index)
        repos_to_query = self.repositories[start_index:end_index]
        code_results = self.search_code_in_repos(lib_query, repos_to_query)

        results_count = code_results.totalCount
        print ("results found: ", results_count )

        if results_count > 0:
          unique_repos = set()
          for code_file in code_results:
            try:
              repo = code_file.repository.full_name
              unique_repos.add(repo)
              if len(unique_repos) == QUERY_SIZE: #found the max num of repos
                break
            except Github:
              Common_Utilities.go_to_sleep("failed, Going to sleep for ", self.quick_sleep)
          #print("unique repos using lib:", num_repos_found)
          frequency =  frequency + len(unique_repos)
          #frequency += self._get_unique_repo_cnt(code_results, num_of_repos)


        start_index += QUERY_SIZE
        end_index = start_index + QUERY_SIZE - 1

        if self._get_search_rate().remaining < 2 == 0:
          Common_Utilities.go_to_sleep("Rate exceeded, Go to sleep for ", self.quick_sleep)          
      except GithubException:
        Common_Utilities.go_to_sleep("Abuse detected... going to sleep for ", self.error_sleep)          
        continue

    print ("done with frequency", frequency)
    return frequency

  def create_output_file(self):
    '''
    Creates the output file (erasing previous content)
    '''
    self.writer = open(self.output_file, "w")  

  def get_repositories(self):
    '''
    returns the available list of repos
    '''
    return self.repositories

  def get_github_obj(self):
    '''
    returns the github3 wrapper object
    '''
    return self.github


def run():
  code_search = CodeSearch()
  code_search.read_repositories()
  code_search.read_libraries()
  code_search.create_output_file()

  for keyword,repo in code_search.libdict.items():  
      import_phrase = "\"import " + keyword + "\"" 
      frequency = code_search.search_code(import_phrase) 
      code_search.send_totals_to_file(repo, frequency )
      Common_Utilities.go_to_sleep("Go to sleep after each library for ", code_search.error_sleep)          

  code_search.close_output_file()

  print ("\n Finally ..... Execution is over \n")

run()

# def main():

#     print("Searching for imports in top repos...")
    
#     config_dict = Common_Utilities.read_config_file() # read all config data
#     repo_array = read_repos()    
    
#     quick_sleep = int (config_dict["QUICK_SLEEP"]) # regular sleep after each iteration
#     error_sleep = int (config_dict["ERROR_SLEEP"]) # Sleep after a serious issue is detected from gitHib, should be around 10min, ie 600 sec 
#     g = None
#     g = Github(config_dict["TOKEN"])   # pass the connection token 
    
#     library_dict = read_libraries(config_dict["LIBRARY_LIST"]) # read all libraries to search against

#     output_file_name = config_dict["POPULARITY_OUTPUT_FILE"] # this is the output file that we are going to send libraries with their total counts to
    
#     output_file = open(output_file_name, "w")  
#     output_file.close()  
    
#     for keyword,repo in library_dict.items():  
#       query = "\"import " + keyword + "\" "  + config_dict["IMPORTSEARCH"]  
#       frequency = search_code_in_repo(query, g, quick_sleep, error_sleep, repo_array) 
#       send_totals_to_file(output_file_name, repo, frequency )
       
#     print ("\n Finally ..... Execution is over \n")    
    
# main()

# #This is where the search happens, an api query is used to collect results. 
# #The query looks like this: "import LIBRARY-NAME" language:java repo:REPO-NAME
# #For each Query, if we get ANY results then that means the library was used in that repo
# #If we get ZERO results, it means it was not used in that repo
# def search_code_in_repo(self, start_index, end_index):

#   repos_to_query = self.repositories[start_index:end_index]   
#   roll_back = True
#   while roll_back:
#     roll_back = False
#     frequency = 0
#     #check github for rate limit 
#     try:
    
#       rate_limit = github.get_rate_limit()
#       rate = rate_limit.search            
#       # this reate limit is not accurate as github may stop you before you reach your limit.
#       print ("search limit: " + str(rate) + ". Reset Time: " + str(rate.reset))
#       if rate.remaining == 0:
#         #print(f'You have 0/{rate.limit} API calls remianing. Reset time: {rate.reset}')            
#         Common_Utilities.go_to_sleep("No more resources to use, Go to sleep for ", error_sleep)  
        
#         query = self.base_query

#         for repo in repos:
#           query += " repo:" + repo

#         msg = str(index) + " out of " + str (arraysize) + " Repos to query : " + query_final
#         print (msg)
#         result = None
#         result = github.search_code(query)
#         num_found = result.totalCount
#         print ("type: ", type(result), "with results: ", num_found )
#         if num_found > 0:
#           unique_repos = set(map(get_repo_name, result))
#           num_repos_found = len(unique_repos)
#           print("size:", num_repos_found)
#           frequency =  frequency + num_repos_found
#           # print(result.repository, result.html_url)      
#         if tracking_counter % 15 == 0:
#           Common_Utilities.go_to_sleep("Force to sleep after each iteration, Go to sleep for ", quick_sleep)          
#         except GithubException:             
#           index = index - 3
#           Common_Utilities.go_to_sleep("Error: Internal abuse detection mechanism detected,Go to sleep for ", error_sleep)
      
#     except GithubException:
#       Common_Utilities.go_to_sleep("Error: abuse detection mechanism detected,Go to sleep for ", error_sleep)
#       roll_back = True # -1 means a problem detected and we need to re-read the same pages again after sleep. no change of date
   
#   return frequency 
   

